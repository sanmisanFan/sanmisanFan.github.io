---
title:          "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering"
date:           2025-07-15 00:01:00 +0800
selected:       true
pub:            "Conference on Language Modeling (COLM 2025)"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
# pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Spotlight</span>'
pub_date:       "2025"

abstract: >-
  Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types: choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.
cover: /assets/images/covers/mapiq.png
authors:
  - Varun Srivastava
  - Fan Lei
  - Srija Mukhopadhyay
  - Vivek Gupta
  - Ross Maciejewski
links:
  # Paper: https://doi.org/10.48550/arXiv.2506.14056
  arXiv: https://doi.org/10.48550/arXiv.2507.11625
  # Presentation: https://youtu.be/m2y69bfE4D8?si=inCuDmGSOZf6QBOh
---
